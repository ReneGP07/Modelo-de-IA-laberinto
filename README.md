# Maze Q-Learning (Proyecto modular)

Refactor de tu script en un proyecto **modular**, con un `main` para ejecutarlo y archivos separados por responsabilidad.
Todo est√° en espa√±ol y mantiene exactamente la misma l√≥gica que tu c√≥digo original.

## Estructura

```
maze_q_learning_project/
‚îú‚îÄ main.py
‚îú‚îÄ requirements.txt
‚îú‚îÄ README.md
‚îî‚îÄ maze_rl/
   ‚îú‚îÄ __init__.py
   ‚îú‚îÄ constants.py
   ‚îú‚îÄ control.py
   ‚îú‚îÄ maze.py
   ‚îú‚îÄ transitions.py
   ‚îú‚îÄ visualize.py
   ‚îú‚îÄ qtransfer.py
   ‚îú‚îÄ training.py
   ‚îú‚îÄ io_utils.py
   ‚îî‚îÄ levels.py
```

## C√≥mo usar

1) (Opcional) Crea un entorno virtual.
2) Instala dependencias:
   ```bash
   pip install -r requirements.txt
   ```

3) Ejecuta el entrenamiento:
   ```bash
   python main.py --niveles 10 --size_min 21 --size_max 81 --seed_base 7 --episodios 4000 --batch 32768 --steps_cap 2048 --watch_every 200 --fps 40 --out_dir checkpoints_maze
   ```

   Tambi√©n puedes ejecutar como m√≥dulo:
   ```bash
   python -m maze_rl.cli  # (si creas un cli.py en el futuro)
   ```

### Controles en la ventana
- **P** o **ESPACIO**: Pausar/Reanudar
- **N** o **‚Üí**: Pasar al siguiente nivel (guarda checkpoint del nivel actual)
- **Q** o **ESC**: Salir **sin** guardar

Los checkpoints se guardan por nivel en `--out_dir`.

## Notas de Backend (Matplotlib)
Se fuerza el backend **TkAgg** para la interfaz interactiva. En algunos entornos remotos o headless puede requerir `sudo apt-get install python3-tk` o usar un backend no interactivo.

---

## ‚ö° Aceleraci√≥n con CUDA (GPU)

Este proyecto **aprovecha n√∫cleos CUDA** cuando hay una **GPU NVIDIA** disponible y **PyTorch** lo detecta:
```python
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# modelo.to(device)
# tensores = tensores.to(device)
```
As√≠, el **entrenamiento por refuerzo** (c√°lculo de valores Q, forward de la red y backprop) se ejecuta en GPU, 
reduciendo significativamente los tiempos de c√≥mputo en escenarios medianos y grandes.

> Si no tienes GPU, el c√≥digo contin√∫a funcionando en CPU sin cambios.

---

## üß† ¬øQu√© modelo uso? (capas + reforzamiento)

El enfoque es **Aprendizaje por Refuerzo** (RL): el agente aprende una **pol√≠tica** que maximiza la recompensa esperada. 
Para aproximar la funci√≥n \( Q(s,a) \), se usan **dos variantes**:

1. **Q-Table** (tabular): √∫til en laberintos peque√±os, discretos. Guarda un valor por cada par (estado, acci√≥n).
2. **Red neuronal (DQN/MLP por capas)**: para espacios m√°s grandes, se usa una **MLP** (perceptr√≥n multicapa) que recibe el estado (p. ej., \((x,y)\) o el grid aplanado) y **predice los Q-values** para cada acci√≥n.
   - Ejemplo de arquitectura: `Entrada ‚Üí [Linear ‚Üí ReLU] √ó 2‚Äì3 ‚Üí Linear(salida=|A|)`
   - **P√©rdida**: MSE entre el objetivo de Q-Learning y la predicci√≥n de la red.
   - **Optimizador**: Adam.
   - **Exploraci√≥n**: Œµ-greedy con **decaimiento de Œµ** a lo largo del entrenamiento.
   - **(Opcional)**: *Experience Replay* y *Target Network* para mayor estabilidad.

> En GPU (CUDA), el **forward/backward** de la red y el c√°lculo del loss se aceleran autom√°ticamente.

---

## üßó Entrenamiento por niveles (10 niveles) y c√≥mo ‚Äúevoluciona‚Äù el modelo

Se aplica un **curriculum learning**: entrenamos el agente **progresivamente** en **10 niveles** de dificultad creciente 
(p. ej., laberintos m√°s grandes, m√°s obst√°culos, recompensas m√°s escasas). El **mismo modelo** se **contin√∫a** entre niveles
(*fine-tuning*), por lo que **no reinicia desde cero** cada vez.

- **Niveles 1‚Äì3**: el agente aprende nociones b√°sicas de navegaci√≥n y evita paredes.
- **Niveles 4‚Äì7**: mejora la **planificaci√≥n** local y reduce pasos redundantes (mejor explotaci√≥n).
- **Niveles 8‚Äì10**: consolida **patrones generales** (atajos, esquinas, t√∫neles) y **generaliza** a variaciones no vistas.

Al finalizar el **nivel 10**, el modelo ha **evolucionado**: necesita **menos exploraci√≥n** para alcanzar metas nuevas,
converge m√°s r√°pido y muestra **transferencia de conocimiento** entre laberintos.

---

## üîÅ ¬øPara qu√© me sirve el entrenamiento en futuras situaciones?

- **Arranque en caliente (warm‚Äëstart)**: reutilizas los **pesos** o la **Q-Table** como punto de partida para **nuevos laberintos**.
- **Menos episodios para converger**: el agente ya ‚Äúsabe‚Äù moverse, explorar y aprovechar recompensas: **aprende m√°s r√°pido**.
- **Transferencia**: patrones aprendidos (evitar cuellos de botella, rodear obst√°culos) **se transfieren** a entornos distintos.
- **Robustez**: ante cambios moderados (tama√±o del grid, obst√°culos), el desempe√±o **se degrada poco** y se recupera con finos ajustes.
- **Producci√≥n/Docencia**: sirve como **pol√≠tica base** para demos, pr√°cticas y retos, reduciendo el tiempo de preparaci√≥n.

> Consejos: guarda checkpoints por nivel (p. ej., `checkpoints/nivel_10.pt`) y exporta las mejores pol√≠ticas para reuso.
